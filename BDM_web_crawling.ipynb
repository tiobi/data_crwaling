{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import csv\n",
    "\n",
    "#크롤링 탐지 우회 헤더 \n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "pages = []\n",
    "urls = []\n",
    "for i in range(1, 77): #페이지 1 ~ n 추가 #다음에서 페이지 수 확인 필수!!\n",
    "    pages.append\\\n",
    "    (\"https://search.daum.net/search?nil_suggest=btn&w=blog&DA=PGD&q=%EC%9D%B4%EB%94%94%EC%95%BC&page=\"\\\n",
    "                 + str(i))\n",
    "\n",
    "for page in pages: #페이지에서 url 가져오기\n",
    "    html = requests.get(page, headers = headers).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    for link in soup.findAll('a', class_=\"f_link_b\"):\n",
    "        if 'href' in link.attrs:\n",
    "            urls.append(link.attrs['href'])\n",
    "#url testing\n",
    "#print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'timeout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2c2581c2dc9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnameList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\anaocnda\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\anaocnda\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'timeout'"
     ]
    }
   ],
   "source": [
    "#url에서 텍스트 크롤링\n",
    "\n",
    "strings = [] \n",
    "for url in urls:\n",
    "\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    nameList = soup.findAll('p')\n",
    "\n",
    "    buff = []\n",
    "    for name in nameList:\n",
    "        buff.append(name.get_text())\n",
    "    strings.append(''.join(buff))\n",
    "\n",
    "    del buff #free buffer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url에서 저자와 날짜 크롤링\n",
    "\n",
    "authors = []\n",
    "dates =  []\n",
    "\n",
    "for url in urls:\n",
    "#     print(url)\n",
    "    htmlL = urlopen(url)\n",
    "    soupL =BeautifulSoup(htmlL, 'html.parser')\n",
    "    \n",
    "    #get author\n",
    "    if soupL.findAll('span', class_=\"author\"):\n",
    "        author = soupL.find('span', class_=\"author\").get_text()        \n",
    "    elif soupL.findAll('span', class_=\"info_post\"):\n",
    "        author = soupL.find('span', class_=\"info_post\").get_text()        \n",
    "    elif soupL.findAll('h1', class_=\"jb-site-title\"):\n",
    "        author = soupL.find('h1', class_=\"jb-site-title\").get_text()        \n",
    "    elif soupL.findAll('button', class_=\"btn_name\"):\n",
    "        author = soupL.find('button', class_=\"btn_name\").get_text()        \n",
    "    elif soupL.findAll('span', class_=\"txt_style\"):\n",
    "        author = soupL.find('span', class_=\"txt_style\").get_text()        \n",
    "    elif soupL.findAll('span', class_=\"txt_bar\"):\n",
    "        author = soupL.find('span', class_=\"txt_bar\").get_text()        \n",
    "    elif soupL.findAll('span', class_=\"p-author h-card\"):\n",
    "        author = soupL.find('span', class_=\"p-author h-card\").get_text()        \n",
    "    elif soupL.findAll('div', class_=\"blank\"):\n",
    "        author = soupL.find('div', class_=\"blank\").get_text()        \n",
    "    elif soupL.findAll('span', itemprop=\"name\"):\n",
    "        author = soupL.find('span', itemprop=\"name\").get_text() \n",
    "    elif soupL.findAll('span', class_=\"title_text\"):\n",
    "        author = soupL.find('span', class_=\"title_text\").get_text()                            \n",
    "    elif soupL.findAll('div', class_=\"name\"):\n",
    "        author = soupL.find('div', class_=\"name\").get_text()        \n",
    "    elif soupL.findAll('a', class_=\"fade-link\"):\n",
    "        author = soupL.find('a', class_=\"fade-link\").get_text()\n",
    "    elif soupL.findAll('div', class_=\"nick\"):\n",
    "        author = soupL.find('div', class_=\"nick\").get_text()\n",
    "        \n",
    "    else:\n",
    "        author = \"## AUTHOR NULL ##\"\n",
    "        \n",
    "    #get date\n",
    "    if soupL.findAll(class_=\"date\"):\n",
    "        date = soupL.find(class_=\"date\").get_text()\n",
    "    elif soupL.findAll(class_=\"text_bar\"):\n",
    "        date = soupL.find(class_=\"text_bar\").get_text()   \n",
    "    elif soupL.findAll('li', class_=\"digit\"):\n",
    "        date = soupL.find('li', class_=\"digit\").get_text() \n",
    "    elif soupL.findAll('span', class_=\"jb-article-information-date\"):\n",
    "        date = soupL.find('span', class_=\"jb-article-information-date\").get_text() \n",
    "    else: \n",
    "        date = \"## DATE NULL ##\"\n",
    "\n",
    "        \n",
    "    author = author.replace('\\n', '').replace('\\t','')\n",
    "    date = date.replace('\\n', '').replace('\\t','')\n",
    "    authors.append(author)\n",
    "    dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns = ['author', 'date', 'text']) #데이터프레임화\n",
    "\n",
    "data['author'] = authors\n",
    "data['date'] = dates\n",
    "data['text'] = strings\n",
    "\n",
    "\n",
    "# UTF-8 인코딩으로 저장 후 .txt 파일로 load 후 ANSI 인코딩 .CSV 파일로 저장!\n",
    "data.to_csv('kw.ediya.csv') #데이터프레임 저장\n",
    "\n",
    "#data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
